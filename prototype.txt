# Open Claw Integration

## GeniOS Brain — 48 Hour Prototype Build Plan

5 segments. Backend only. OpenClaw integration. Zero noise.

---

## Pre-Start — Before You Write One Line of Code (30 mins)

Set up your environment first. Do this before anything else or you will waste hours mid-build.

**Accounts and keys needed right now:**

- Anthropic API key — claude-sonnet-4-5 and haiku both
- Qdrant Cloud account — free tier, create one cluster, save the URL and API key
- Supabase account — free tier, create one project, save connection string
- OpenClaw / Molt Bot account — get API access and webhook capability confirmed
- Railway.app account — for deployment in segment 4

**Local setup:**

```bash
mkdir genios-brain && cd genios-brain
python -m venv venv && source venv/activate
pip install fastapi uvicorn anthropic qdrant-client
pip install supabase python-dotenv httpx sentence-transformers
```

Create your `.env` file immediately:

```
ANTHROPIC_API_KEY=
QDRANT_URL=
QDRANT_API_KEY=
SUPABASE_URL=
SUPABASE_KEY=
OPENAI_API_KEY=  # optional fallback
ORG_ID=genios_internal
```

**Your folder structure — lock this now, do not deviate:**

```
genios-brain/
├── main.py              # FastAPI app, all routes
├── context/
│   ├── retriever.py     # vector + structured fetch
│   └── store.py         # write context to DBs
├── reasoning/
│   └── engine.py        # Claude reasoning calls
├── integrations/
│   └── openclaw.py      # OpenClaw specific formatting
├── data/
│   └── seed.py          # your org data loader
├── .env
└── requirements.txt
```

---

## Segment 1 — Context Store and Seed Data (Hours 0-8)

**Goal:** GeniOS knows who you are, what GeniOS is, your relationships, your policies. All queryable. No retrieval calls yet — just build and populate the store.

---

**Step 1.1 — Supabase Schema (1 hour)**

Open Supabase SQL editor. Run this:

```sql
-- Organization profile
create table org_context (
  id uuid default gen_random_uuid() primary key,
  org_id text not null,
  context_type text not null, -- 'profile', 'policy', 'relationship', 'decision'
  entity_name text,           -- person, company, project name if applicable
  content text not null,      -- the actual context in plain text
  metadata jsonb,             -- flexible extra fields
  created_at timestamp default now()
);

-- Active entity states
create table entity_state (
  id uuid default gen_random_uuid() primary key,
  org_id text not null,
  entity_type text not null,  -- 'investor', 'client', 'vendor', 'project', 'task'
  entity_name text not null,
  current_state jsonb not null,
  last_updated timestamp default now()
);

-- Interaction log
create table interaction_log (
  id uuid default gen_random_uuid() primary key,
  org_id text not null,
  intent text not null,
  raw_message text,
  context_used jsonb,
  enriched_output text,
  verdict text,
  confidence float,
  created_at timestamp default now()
);

create index on org_context(org_id, context_type);
create index on entity_state(org_id, entity_type, entity_name);
```

**Step 1.2 — Qdrant Collection (30 mins)**

```python
# run this once as setup script
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
import os

client = QdrantClient(
    url=os.getenv("QDRANT_URL"),
    api_key=os.getenv("QDRANT_API_KEY")
)

client.create_collection(
    collection_name="genios_context",
    vectors_config=VectorParams(size=384, distance=Distance.COSINE)
    # 384 = all-MiniLM-L6-v2 dimensions, fast and free
)
print("Collection created")
```

**Step 1.3 — Seed Your Org Data (3-4 hours)**

This is the most important step of segment 1. Populate real data. Your actual context. Not dummy data.

Build `data/seed.py`:

```python
from supabase import create_client
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct
from sentence_transformers import SentenceTransformer
import uuid, os
from dotenv import load_dotenv

load_dotenv()

supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))
qdrant = QdrantClient(url=os.getenv("QDRANT_URL"), api_key=os.getenv("QDRANT_API_KEY"))
encoder = SentenceTransformer('all-MiniLM-L6-v2')

ORG_ID = "genios_internal"

def store_context(context_type, content, entity_name=None, metadata={}):
    # Store in Supabase
    supabase.table("org_context").insert({
        "org_id": ORG_ID,
        "context_type": context_type,
        "entity_name": entity_name,
        "content": content,
        "metadata": metadata
    }).execute()

    # Store in Qdrant for semantic search
    vector = encoder.encode(content).tolist()
    qdrant.upsert(
        collection_name="genios_context",
        points=[PointStruct(
            id=str(uuid.uuid4()),
            vector=vector,
            payload={
                "org_id": ORG_ID,
                "context_type": context_type,
                "entity_name": entity_name,
                "content": content,
                "metadata": metadata
            }
        )]
    )
    print(f"Stored: {context_type} - {entity_name or 'general'}")

# ============================================
# SEED YOUR ACTUAL DATA BELOW
# ============================================

# 1. Organization Profile
store_context("profile", """
GeniOS Brain is a decision and cognition layer for agentic AI systems.
We are pre-seed stage, building the governance and intelligence layer
that sits between AI orchestrators and agents. Founded by [your name].
Current focus: prototype validation with internal testing on OpenClaw.
Stage: pre-seed, building prototype.
Team: [your team details].
Location: India.
""")

# 2. Policies — write your actual operating policies
policies = [
    "Always personalize investor outreach based on their portfolio thesis before sending",
    "Follow up with investors maximum once every 6 days unless they respond",
    "Never share financial projections without founder approval first",
    "All external communications must reference latest product update",
    "Escalate to founder when any investor responds positively or requests a meeting",
    "Do not follow up with investors who have explicitly said no in last 90 days",
]
for policy in policies:
    store_context("policy", policy)

# 3. Key Relationships — your actual investors, clients, partners
relationships = [
    {
        "name": "Investor Name 1",
        "entity_name": "Investor Name 1",
        "content": """
        Investor at [Firm]. Focus areas: B2B SaaS, AI infrastructure, developer tools.
        Portfolio includes: [relevant companies].
        Last contact: [date]. Last interaction: [what happened].
        Status: warm / cold / no response.
        Notes: [anything specific about this person].
        """
    },
    # Add all your actual investor contacts here
    # Add clients, partners, vendors the same way
]
for rel in relationships:
    store_context("relationship", rel["content"], entity_name=rel["name"])

# 4. Entity States — current status of key relationships
entity_states = [
    {
        "entity_type": "investor",
        "entity_name": "Investor Name 1",
        "current_state": {
            "status": "warm",
            "last_contact_days_ago": 12,
            "follow_up_due": True,
            "next_action": "share product update",
            "meeting_scheduled": False
        }
    },
]

for state in entity_states:
    supabase.table("entity_state").insert({
        "org_id": ORG_ID,
        **state
    }).execute()

print("All seed data loaded")
```

Run this: `python data/seed.py`

Verify data loaded correctly by checking Supabase table and doing a test Qdrant search.

**Step 1.4 — Verify Your Store (30 mins)**

Write a quick test before moving on:

```python
# test_retrieval.py - run this to verify
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
import os
from dotenv import load_dotenv
load_dotenv()

client = QdrantClient(url=os.getenv("QDRANT_URL"), api_key=os.getenv("QDRANT_API_KEY"))
encoder = SentenceTransformer('all-MiniLM-L6-v2')

test_intents = [
    "follow up with investors",
    "what is our policy on investor communication",
    "who should i reach out to today",
    "what is genios brain",
]

for intent in test_intents:
    vector = encoder.encode(intent).tolist()
    results = client.search(
        collection_name="genios_context",
        query_vector=vector,
        limit=3,
        query_filter={"must": [{"key": "org_id", "match": {"value": "genios_internal"}}]}
    )
    print(f"\nIntent: {intent}")
    for r in results:
        print(f"  Score: {r.score:.3f} | {r.payload['content'][:100]}")
```

Do not move to Segment 2 until retrieval results look correct. Fix seed data if results are bad.

---

## Segment 2 — Context Retriever and Reasoning Engine (Hours 8-20)

**Goal:** Given any intent, fetch the right context and reason over it. The two core brain functions working end to end.

---

**Step 2.1 — Build the Retriever (2 hours)**

`context/retriever.py`:

```python
from qdrant_client import QdrantClient
from supabase import create_client
from sentence_transformers import SentenceTransformer
import os

class ContextRetriever:
    def __init__(self):
        self.qdrant = QdrantClient(
            url=os.getenv("QDRANT_URL"),
            api_key=os.getenv("QDRANT_API_KEY")
        )
        self.supabase = create_client(
            os.getenv("SUPABASE_URL"),
            os.getenv("SUPABASE_KEY")
        )
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')

    def get_context(self, intent: str, org_id: str, entity_name: str = None) -> dict:
        context = {
            "semantic_context": [],
            "entity_state": None,
            "policies": [],
            "org_profile": None
        }

        # 1. Semantic search — find relevant context chunks
        vector = self.encoder.encode(intent).tolist()
        results = self.qdrant.search(
            collection_name="genios_context",
            query_vector=vector,
            limit=6,
            query_filter={
                "must": [{"key": "org_id", "match": {"value": org_id}}]
            }
        )

        for r in results:
            if r.score > 0.35:  # relevance threshold, tune this
                if r.payload["context_type"] == "policy":
                    context["policies"].append(r.payload["content"])
                elif r.payload["context_type"] == "profile":
                    context["org_profile"] = r.payload["content"]
                else:
                    context["semantic_context"].append({
                        "content": r.payload["content"],
                        "score": round(r.score, 3),
                        "type": r.payload["context_type"],
                        "entity": r.payload.get("entity_name")
                    })

        # 2. Direct entity state fetch if entity mentioned
        if entity_name:
            result = self.supabase.table("entity_state")\
                .select("*")\
                .eq("org_id", org_id)\
                .ilike("entity_name", f"%{entity_name}%")\
                .execute()
            if result.data:
                context["entity_state"] = result.data[0]["current_state"]

        return context
```

**Step 2.2 — Build the Reasoning Engine (3-4 hours)**

This is where you will spend the most time. The prompt is everything.

`reasoning/engine.py`:

```python
import anthropic
import json
import os

class ReasoningEngine:
    def __init__(self):
        self.client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

    def classify_intent(self, message: str) -> dict:
        # Fast classification using Haiku - cheap and fast
        response = self.client.messages.create(
            model="claude-haiku-4-5-20251001",
            max_tokens=200,
            messages=[{
                "role": "user",
                "content": f"""Classify this user message into:
- intent_type: one of [outreach, followup, information_fetch, task_creation, policy_check, decision_request, general]
- entity_name: the specific person/company/project mentioned (null if none)
- requires_deep_reasoning: true/false (true if consequential action, false if simple info)
- domain: one of [investor_relations, sales, operations, internal, hr, finance]

Message: "{message}"

Return JSON only, no explanation."""
            }]
        )

        try:
            return json.loads(response.content[0].text)
        except:
            return {
                "intent_type": "general",
                "entity_name": None,
                "requires_deep_reasoning": True,
                "domain": "internal"
            }

    def enrich_and_reason(self, intent: str, context: dict, classification: dict) -> dict:
        # Build the reasoning prompt
        prompt = self._build_prompt(intent, context, classification)

        # Choose model based on complexity
        model = "claude-sonnet-4-5" if classification.get("requires_deep_reasoning") else "claude-haiku-4-5-20251001"

        response = self.client.messages.create(
            model=model,
            max_tokens=1000,
            messages=[{"role": "user", "content": prompt}]
        )

        try:
            result = json.loads(response.content[0].text)
            return result
        except:
            # If JSON parsing fails, return structured error
            return {
                "verdict": "ERROR",
                "enriched_brief": response.content[0].text,
                "flags": ["JSON parsing failed - check prompt"],
                "confidence": 0.0
            }

    def _build_prompt(self, intent: str, context: dict, classification: dict) -> str:

        semantic_chunks = "\n".join([
            f"- [{c['type']}] {c['content'][:300]}"
            for c in context.get("semantic_context", [])
        ])

        policies = "\n".join([f"- {p}" for p in context.get("policies", [])])

        entity_state = json.dumps(context.get("entity_state"), indent=2) \
            if context.get("entity_state") else "No specific entity state found"

        org_profile = context.get("org_profile", "No org profile loaded")

        return f"""You are GeniOS Brain - the cognitive layer for an AI agent system.

Your job is to analyze the user's intent, reason over the organizational context provided, and return an enriched brief that makes the executing AI agent dramatically more effective.

USER INTENT: {intent}
DOMAIN: {classification.get('domain')}
ENTITY: {classification.get('entity_name', 'Not specified')}

ORGANIZATION PROFILE:
{org_profile}

RELEVANT CONTEXT:
{semantic_chunks if semantic_chunks else "No specific context retrieved"}

CURRENT ENTITY STATE:
{entity_state}

ACTIVE POLICIES:
{policies if policies else "No policies retrieved"}

INSTRUCTIONS:
1. Reason over all context provided
2. Identify what the agent needs to know to execute this intent excellently
3. Flag any policy violations or concerns
4. Determine if this needs human escalation or can proceed autonomously
5. Return a specific, personalized enriched brief - not generic

Return ONLY valid JSON in this exact format:
{{
  "verdict": "PROCEED" | "ESCALATE" | "BLOCK" | "CLARIFY",
  "enriched_brief": "Specific, detailed brief for the executing agent. Include exact names, context, tone guidance, what to reference, what to avoid. This should be 3-5 sentences minimum and highly specific.",
  "key_context_used": ["list", "of", "key", "facts", "you", "used"],
  "flags": ["any", "policy", "violations", "or", "concerns"],
  "recommended_action": "Specific next action the agent should take",
  "escalate_reason": null,
  "confidence": 0.85
}}"""
```

**Step 2.3 — Test Reasoning in Isolation (2 hours)**

Do not connect to OpenClaw yet. Test reasoning alone.

```python
# test_reasoning.py
from context.retriever import ContextRetriever
from reasoning.engine import ReasoningEngine
import json
from dotenv import load_dotenv
load_dotenv()

retriever = ContextRetriever()
engine = ReasoningEngine()
ORG_ID = "genios_internal"

# Test these 10 intents against your real data
test_cases = [
    "follow up with the investors who haven't responded in 7 days",
    "draft an update email about our prototype progress",
    "who should we prioritize reaching out to today",
    "check if we can share our financial projections with a new investor",
    "what is the status of our investor pipeline",
    "remind me what we discussed with [specific investor name]",
    "send a follow up to [specific investor] about the meeting",
    "what is our policy on investor communication frequency",
    "create a task to follow up with all warm investors this week",
    "what should i say to an investor asking about our traction",
]

for intent in test_cases:
    print(f"\n{'='*60}")
    print(f"INTENT: {intent}")

    classification = engine.classify_intent(intent)
    print(f"CLASSIFICATION: {json.dumps(classification, indent=2)}")

    context = retriever.get_context(
        intent,
        ORG_ID,
        entity_name=classification.get("entity_name")
    )

    result = engine.enrich_and_reason(intent, context, classification)
    print(f"RESULT: {json.dumps(result, indent=2)}")
```

Run this. Read every output carefully. For each bad output ask: is the context retrieval wrong? Is the prompt wrong? Is the seed data missing something? Fix accordingly. Run again. Repeat until 8/10 outputs are genuinely good.

This iteration loop is your most important work in the entire 48 hours.

---

## Segment 3 — FastAPI Endpoints and Core Loop (Hours 20-32)

**Goal:** Working API that accepts intents, returns enriched verdicts. The full loop in one HTTP call.

---

**Step 3.1 — Main Application (2 hours)**

`main.py`:

```python
from fastapi import FastAPI, HTTPException, Header
from pydantic import BaseModel
from typing import Optional
from context.retriever import ContextRetriever
from reasoning.engine import ReasoningEngine
from supabase import create_client
import os, json
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

app = FastAPI(title="GeniOS Brain - Prototype")

retriever = ContextRetriever()
engine = ReasoningEngine()
supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

# ============================================
# REQUEST / RESPONSE MODELS
# ============================================

class EnrichRequest(BaseModel):
    org_id: str
    raw_message: str
    entity_name: Optional[str] = None
    current_step: Optional[str] = None
    agent_id: Optional[str] = "openclaw"

class EnrichResponse(BaseModel):
    verdict: str
    enriched_brief: str
    recommended_action: str
    flags: list
    confidence: float
    key_context_used: list
    decision_id: str

# ============================================
# MAIN ENDPOINT — This is your entire product
# ============================================

@app.post("/v1/enrich", response_model=EnrichResponse)
async def enrich_context(request: EnrichRequest):
    try:
        # Step 1: Classify intent fast
        classification = engine.classify_intent(request.raw_message)

        # Step 2: Fetch relevant context
        entity = request.entity_name or classification.get("entity_name")
        context = retriever.get_context(
            intent=request.raw_message,
            org_id=request.org_id,
            entity_name=entity
        )

        # Step 3: Reason and enrich
        result = engine.enrich_and_reason(
            intent=request.raw_message,
            context=context,
            classification=classification
        )

        # Step 4: Log interaction
        decision_id = f"dec_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        supabase.table("interaction_log").insert({
            "org_id": request.org_id,
            "intent": request.raw_message,
            "context_used": context,
            "enriched_output": result.get("enriched_brief"),
            "verdict": result.get("verdict"),
            "confidence": result.get("confidence", 0)
        }).execute()

        return EnrichResponse(
            verdict=result.get("verdict", "PROCEED"),
            enriched_brief=result.get("enriched_brief", ""),
            recommended_action=result.get("recommended_action", ""),
            flags=result.get("flags", []),
            confidence=result.get("confidence", 0.0),
            key_context_used=result.get("key_context_used", []),
            decision_id=decision_id
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============================================
# UTILITY ENDPOINTS
# ============================================

@app.post("/v1/store-context")
async def store_new_context(
    org_id: str,
    context_type: str,
    content: str,
    entity_name: Optional[str] = None
):
    """Add new context on the fly during testing"""
    from context.store import store_context
    store_context(org_id, context_type, content, entity_name)
    return {"status": "stored", "org_id": org_id}

@app.get("/v1/logs/{org_id}")
async def get_interaction_logs(org_id: str, limit: int = 20):
    """Review what GeniOS has been doing"""
    result = supabase.table("interaction_log")\
        .select("*")\
        .eq("org_id", org_id)\
        .order("created_at", desc=True)\
        .limit(limit)\
        .execute()
    return result.data

@app.get("/health")
async def health():
    return {"status": "alive", "service": "GeniOS Brain Prototype"}
```

**Step 3.2 — Context Store Helper (30 mins)**

`context/store.py`:

```python
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct
from supabase import create_client
from sentence_transformers import SentenceTransformer
import uuid, os

def store_context(org_id: str, context_type: str, content: str, entity_name: str = None):
    encoder = SentenceTransformer('all-MiniLM-L6-v2')
    qdrant = QdrantClient(url=os.getenv("QDRANT_URL"), api_key=os.getenv("QDRANT_API_KEY"))
    supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

    supabase.table("org_context").insert({
        "org_id": org_id,
        "context_type": context_type,
        "entity_name": entity_name,
        "content": content
    }).execute()

    vector = encoder.encode(content).tolist()
    qdrant.upsert(
        collection_name="genios_context",
        points=[PointStruct(
            id=str(uuid.uuid4()),
            vector=vector,
            payload={
                "org_id": org_id,
                "context_type": context_type,
                "entity_name": entity_name,
                "content": content
            }
        )]
    )
```

**Step 3.3 — Run and Test API Locally (1 hour)**

```bash
uvicorn main:app --reload --port 8000
```

Test every endpoint with curl or Postman:

```bash
# Main enrich call
curl -X POST http://localhost:8000/v1/enrich \
  -H "Content-Type: application/json" \
  -d '{
    "org_id": "genios_internal",
    "raw_message": "follow up with the investors who havent responded",
    "agent_id": "openclaw_test"
  }'

# Check logs
curl http://localhost:8000/v1/logs/genios_internal
```

Run 15 different calls. Read every response. Fix anything that looks wrong in retriever or engine. Do not proceed to OpenClaw integration until the API is returning consistently good enriched briefs.

---

## Segment 4 — OpenClaw Integration (Hours 32-40)

**Goal:** OpenClaw calls GeniOS before executing actions. GeniOS enriched brief becomes OpenClaw's working context. The full loop working end to end.

---

**Step 4.1 — Deploy to Railway (1 hour)**

You need a public URL for OpenClaw to call. Railway is fastest.

```bash
# Install Railway CLI
npm install -g @railway/cli

# In your project folder
railway login
railway init
railway up
```

Set your environment variables in Railway dashboard — copy all from your `.env` file. Railway gives you a public URL immediately. Your endpoint is now at `https://your-app.railway.app/v1/enrich`.

Test it is live:

```bash
curl https://your-app.railway.app/health
```

**Step 4.2 — OpenClaw Tool Definition (1 hour)**

In OpenClaw / Molt Bot, you need to define GeniOS as a custom tool the bot calls before executing any task. The exact UI for this depends on OpenClaw's current interface but the pattern is:

Create a custom tool called `genios_brain` with this configuration:

```json
{
  "tool_name": "genios_brain",
  "description": "Call GeniOS Brain before any task execution to get enriched context, personalized guidance, and policy validation. Always call this first for any outreach, follow-up, decision, or multi-step task.",
  "endpoint": "https://your-app.railway.app/v1/enrich",
  "method": "POST",
  "headers": {"Content-Type": "application/json"},
  "body_template": {
    "org_id": "genios_internal",
    "raw_message": "{{user_message}}",
    "entity_name": "{{entity_if_mentioned}}",
    "agent_id": "openclaw"
  }
}
```

If OpenClaw does not support custom tool definitions directly, use their webhook or system prompt injection approach:

Add this to your OpenClaw system prompt:

```
Before executing any task that involves:
- Reaching out to someone
- Following up with anyone
- Making a decision
- Any multi-step workflow

You MUST first call the GeniOS Brain API:
POST https://your-app.railway.app/v1/enrich
Body: {"org_id": "genios_internal", "raw_message": "[the user's request]"}

Use the "enriched_brief" from the response as your full context for the task.
Follow the "recommended_action" exactly.
If verdict is "ESCALATE", tell the user this needs their direct attention.
If verdict is "BLOCK", explain the policy reason and do not proceed.
```

**Step 4.3 — OpenClaw Webhook Endpoint (1 hour)**

If OpenClaw fires webhooks on task completion, add this endpoint to capture outcomes:

```python
@app.post("/v1/openclaw-webhook")
async def openclaw_outcome(payload: dict):
    """Capture what OpenClaw did and store as new context"""

    # Extract what happened
    task_completed = payload.get("task_description")
    outcome = payload.get("result")
    entities_involved = payload.get("entities", [])

    if task_completed and outcome:
        # Store this as new context for future retrieval
        from context.store import store_context
        store_context(
            org_id="genios_internal",
            context_type="decision",
            content=f"Task: {task_completed}. Outcome: {outcome}",
            entity_name=entities_involved[0] if entities_involved else None
        )

    return {"status": "logged"}
```

**Step 4.4 — End to End Test with Real OpenClaw Prompts (2-3 hours)**

Run these actual prompts in OpenClaw and watch what happens:

```
Test 1: "follow up with all investors who haven't responded in the last week"
Expected: GeniOS returns enriched brief with specific investor names,
their status, recommended message tone, what update to share

Test 2: "send an update about our prototype progress to the team"
Expected: GeniOS enriches with current prototype context,
who the team members are, what to highlight

Test 3: "should I share our deck with a new investor who just asked?"
Expected: GeniOS checks policy, returns PROCEED or ESCALATE
with specific guidance

Test 4: "what's the status of our investor pipeline"
Expected: GeniOS retrieves entity states, returns current picture

Test 5: "reach out to [specific investor name] about scheduling a demo"
Expected: GeniOS returns everything about that specific investor,
their past interaction, recommended approach
```

For each test, compare two things: what OpenClaw would have done without GeniOS versus what it does with the enriched brief. The difference is your proof of concept.

Document every result. Screenshot or log every enriched brief and the resulting OpenClaw output.

---

## Segment 5 — Validation and Documentation (Hours 40-48)

**Goal:** Prove the prototype works. Structured test results. Decisions made about what to fix before MVP.

---

**Step 5.1 — Structured Validation Test (3 hours)**

Run 20 test cases against your prototype. Use this exact scoring sheet:

For each test case score these four things from 1-5:

Context accuracy — did GeniOS retrieve the right context? Reasoning quality — did the enriched brief make sense and use the context well? Output improvement — was OpenClaw's output noticeably better with GeniOS versus without? Policy handling — did policy-relevant cases get flagged correctly?

Your prototype passes validation if average score across all four is above 3.5 out of 5. If below, identify the weakest area and fix it before MVP.

**Step 5.2 — Log Analysis (1 hour)**

Pull your interaction logs:

```bash
curl https://your-app.railway.app/v1/logs/genios_internal?limit=50
```

Look for patterns: which intents returned low confidence? Which retrieval results had low scores? Which outputs did the reasoning engine struggle with? These are your MVP fix list.

**Step 5.3 — What To Fix List**

Create three lists right now:

Fix before MVP — things that are broken and will block the pilot startups. Context missing for a key domain, reasoning producing wrong verdicts, API too slow.

Fix during MVP — things that are suboptimal but workable. Retrieval returning slightly irrelevant chunks, enriched briefs could be more specific, logging needs more fields.

Fix post-MVP — things that are polish. Better UI, faster response time, more sophisticated policy engine.

**Step 5.4 — Cost Baseline**

Check your Anthropic API usage dashboard. After 20-30 test calls you will have a baseline cost per call. Calculate:

Average cost per enrich call. Fast path cost (haiku) versus slow path cost (sonnet). What 1000 calls per day costs you. What you need to charge per call to get 3x markup.

This is your pricing input for the pilot charging model.

**Step 5.5 — What You Now Have**

After 48 hours you have:

A working backend that accepts any intent, retrieves relevant organizational context, reasons over it, and returns a structured enriched brief. OpenClaw calling GeniOS before executing tasks and producing noticeably better outputs. Your own real organizational data powering the context layer. Interaction logs showing every call, what context was used, what verdict was given. A clear list of what works, what needs fixing, and what is missing for MVP.

That is your prototype. That is enough to validate the core idea and move to the 15-pilot MVP build.

---

## The Single Most Important Thing in These 48 Hours

Do not skip the prompt iteration in Segment 2.

Everything depends on the quality of your reasoning prompt. Bad prompt means bad enriched briefs regardless of how good your retrieval is. Spend a disproportionate amount of time on that prompt. Run it 20 times. Read every output. Make it specific, make it structured, make it return exactly what OpenClaw needs to do its job better.

The prompt is your product at prototype stage. Treat it accordingly.